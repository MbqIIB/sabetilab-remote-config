- hosts: nodes

  environment:
    PLACEHOLDER_VAR: "foobar"

  vars:
    security_ssh_port: "{{ ssh_port }}"
    autossh_tunnel_client_identity: autossh_id_rsa
    autossh_tunnel_client_key_map:
      - src: "./files/{{autossh_tunnel_client_identity}}"
    autossh_tunnel_client_host: "{{ manager_domain_name }}"
    autossh_tunnel_client_forward: '{{ ssh_tunnel_port }}:127.0.0.1:{{ ssh_port }}'

  vars_files:
    - "./settings.yml"

  roles:
    - geerlingguy.security
    - tersmitten.autossh-tunnel-client

  tasks:
    - name: Install htop
      apt: pkg=htop state=installed

    - name: Install git
      apt: pkg=git state=installed

    - name: Install ansible
      apt: pkg=ansible state=installed

    - name: Install python-dev
      apt: pkg=python-dev state=installed

    - name: Install pip
      apt: pkg=python-pip state=installed

    - name: Create file copy destination on remote
      file: path=./files state=directory mode=0755

    - name: Copy python dependency file 
      copy: 
        src: ./files/requirements.txt
        dest: ./files/python-requirements.txt 
        mode: 0644

    - name: Install python dependencies
      pip: requirements=./files/python-requirements.txt

    - name: Copy in Route53 update script (as template)
      template: 
        src: ./files/update_route53.py.j2
        dest: ./files/update_route53.py
        mode: 0744

    - name: setup cron job to update Route53 on reboot
      cron: 
        name: update route53 (on reboot)
        special_time: reboot
        job: ./files/update_route53.py

    - name: setup cron job to update Route53 every half-hour
      cron: 
        name: update route53
        minute: 5,35
        job: ./files/update_route53.py

    - name: update the AWS Route53 subdomain for this node (hostname) upon ansible setup
      route53: 
        aws_access_key: "{{ aws_access_key_id }}" 
        aws_secret_key: "{{ aws_secret_access_key }}"
        command: create
        overwrite: yes
        zone: "{{ domain_name }}"
        hosted_zone_id: "{{ aws_hosted_zone_id }}"
        record: "{{ ansible_nodename }}.{{ domain_name }}"
        type: A
        ttl: 7200
        value: "{{ ansible_eth0.ipv4.address }}"

    - name: add users 
      user: "name={{item|quote}} shell=/bin/bash"
      with_items: "{{github_usernames_with_access}}"

    - name: add users with sudo access
      user: "name={{item|quote}} group=sudo shell=/bin/bash" # use "admin" for Ubuntu <11.10, and "sudo" for later
      with_items: "{{github_usernames_with_sudo_access}}"

    - name: add ssh keys for users from GitHub
      authorized_key: "user={{item|quote}} key=https://github.com/{{item}}.keys"
      with_items: "{{github_usernames_with_access}}"

    - name: add ssh keys for sudo users from GitHub
      authorized_key: "user={{item|quote}} key=https://github.com/{{item}}.keys"
      with_items: "{{github_usernames_with_sudo_access}}"

    - name: Copy in password clearing script
      template: 
        src: "../files/clear_and_expire_password.sh" 
        dest: "./files/clear_and_expire_password.sh"
        mode: "0744"

    - name: Clear and expire passwords so sudoers are promped at first (key-based) login
      command: "./files/clear_and_expire_password.sh {{item}}"
      with_items: "{{github_usernames_with_sudo_access}}"

    - name: setup cron job to call ansible-pull every hour
      cron: 
        name: "update route53"
        minute: "5"
        job: "ansible-pull -i ./production ./field-node/base_playbook.yml"
